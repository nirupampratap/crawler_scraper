{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl & Scrape any Website\n",
    "#### Crawling Eenadu.net in this example\n",
    "**Notebook Author**: Nirupam Purushothama\n",
    "\n",
    "**Note:**\n",
    "* This notebook crawls and scrapes a popular Telugu daily Eenadu.net\n",
    "* Notebook should be modified to custom suit the crawl requirements of individual websites (i.e. based on link structures)\n",
    "* Notebook provides code to download image files (you can easily extend this to video files as well)\n",
    "* Notebook stores the crawled text in a json file. Provides the ability to link text to urls\n",
    "* Notebook does not handle Captchas. Throttle the requests to not run into captchas. Else integrate with a Captcha decoder to auto answer captchas (this is work for a later date)\n",
    "\n",
    "Happy crawling!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "import shutil\n",
    "from contextlib import closing\n",
    "import hashlib\n",
    "import json\n",
    "import os.path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_get(url):\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "            \n",
    "    except RequestException as e:\n",
    "        log_error('Error during request to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "    \n",
    "def is_good_response(resp):\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200\n",
    "           and content_type is not None\n",
    "           and content_type.find('html') > -1)\n",
    "\n",
    "def log_error(e):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_loc = \"./dump/\"\n",
    "\n",
    "def download_file(url):\n",
    "    fname = url.split('/')[-1]\n",
    "    local_filename = folder_loc + fname\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    while(os.path.isfile(local_filename) == True):\n",
    "        local_filename = folder_loc + str(counter) + fname\n",
    "        counter += 1\n",
    "    \n",
    "    with get(url, stream=True) as r:\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_img(raw_html, url=None):\n",
    "    all_text = \"\"\n",
    "    \n",
    "    try:\n",
    "        bs_html = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "        imgdiv = bs_html.find('section', attrs={'id': \"content-smart\"})\n",
    "\n",
    "        if(imgdiv == None):\n",
    "            return\n",
    "\n",
    "        elements_to_grab = ['img']\n",
    "\n",
    "        for ele in elements_to_grab:\n",
    "            for p in imgdiv.select(ele):\n",
    "                img_url = p[\"src\"]\n",
    "                if(img_url != None):\n",
    "                    download_file(img_url)\n",
    "                    # Use sleep to throttle the number of requests to server per minute/second. \n",
    "                    # There is a possibility of hitting a captcha if we do not throttle requests.\n",
    "                    #time.sleep(0.25)\n",
    "                \n",
    "    except TypeError as e:\n",
    "        print(\"\")\n",
    "        print(\"Exception: \" + str(e) + \" , URL: \" + url)\n",
    "        \n",
    "    return None\n",
    "\n",
    "def extract_text(raw_html, url=None):\n",
    "    all_text = \"\"\n",
    "    \n",
    "    try:\n",
    "        bs_html = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "        sectiondiv = bs_html.find('section', attrs={'id': \"content-smart\"})\n",
    "\n",
    "        if(sectiondiv == None):\n",
    "            return\n",
    "\n",
    "        elements_to_grab = ['p']\n",
    "\n",
    "        for ele in elements_to_grab:\n",
    "            for p in sectiondiv.select(ele):\n",
    "                p_text = p.text\n",
    "                if(p_text != None):\n",
    "                    all_text += p_text\n",
    "                    \n",
    "    except TypeError as e:\n",
    "        print(\"\")\n",
    "        print(\"Exception: \" + str(e) + \" , URL: \" + url)\n",
    "        \n",
    "    return all_text\n",
    "\n",
    "def check_valid_url(url_string):\n",
    "    isvalid = True\n",
    "    \n",
    "    if(url_string.find(\"eenadu.net\") == -1 or url_string.find(\"eenadu.net\") > 10):\n",
    "        isvalid = False\n",
    "        \n",
    "    return isvalid\n",
    "\n",
    "def make_url_useful(url_string):\n",
    "    if(url_string.startswith(\"http\") == True):\n",
    "        return url_string\n",
    "    elif(url_string.startswith(\"//\") == True):\n",
    "        return \"https:\"+url_string\n",
    "    else:\n",
    "        return \"https://\"+url_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_to_url_map = {}\n",
    "hash_to_text_map = {}\n",
    "\n",
    "def crawl_scrape(url_proc_map, url_proc_stack, root_url):\n",
    "    \n",
    "    depth = 0\n",
    "    deep = True\n",
    "    extract_external_info = False\n",
    "    data_dict = {}\n",
    "    url_dict = {}\n",
    "    \n",
    "    # Extract only from 10 pages. You can increase the count to whatever you like\n",
    "    n_steps = 10\n",
    "    \n",
    "    # Function handles only a open file handle\n",
    "    \n",
    "    while(len(url_proc_stack) > 0):\n",
    "        depth += 1\n",
    "        \n",
    "        if(depth > n_steps):\n",
    "            print(\"Reached crawl limit.\")\n",
    "            break\n",
    "        \n",
    "        full_url = url_proc_stack.pop()\n",
    "        \n",
    "        # Create the md5 hashcode for dictionary\n",
    "        hash_code = hashlib.md5(full_url.encode())\n",
    "        \n",
    "        # Mark the URL as processed\n",
    "        url_proc_map[hash_code.hexdigest()] = 1\n",
    "        \n",
    "        # Print status\n",
    "        # print(\"Extracting \", full_url)\n",
    "        print(\"=\", end=\"\")\n",
    "        \n",
    "        # Extract HTML\n",
    "        rw_html = simple_get(full_url)\n",
    "        \n",
    "        # If there was an error reading that url then just continue\n",
    "        if(rw_html == None):\n",
    "            continue\n",
    "        \n",
    "        page_text = extract_text(rw_html, full_url)\n",
    "        \n",
    "        # Proceed to save\n",
    "        if(page_text != None and page_text != \"\"):\n",
    "            hash_to_url_map[hash_code.hexdigest()] = full_url\n",
    "            hash_to_text_map[hash_code.hexdigest()] = page_text\n",
    "        \n",
    "        # Extract more urls from this text blob\n",
    "        bs_html = BeautifulSoup(rw_html, 'html.parser')\n",
    "        \n",
    "        if(deep == True):\n",
    "            for a in bs_html.find_all('a', href=True):\n",
    "                \n",
    "                if(a == None):\n",
    "                    continue\n",
    "                \n",
    "                url_part_string = root_url + a['href']\n",
    "                \n",
    "                # If invalid URL then skip adding the url\n",
    "                if(check_valid_url(url_part_string) == False):\n",
    "                    continue\n",
    "                    \n",
    "                url_part_string = make_url_useful(url_part_string)\n",
    "                \n",
    "                hash_code = hashlib.md5(url_part_string.encode())\n",
    "\n",
    "                #Add to stack if part url is not already a part of the proc_map else ignore\n",
    "                if((hash_code.hexdigest() in url_proc_map) == False):\n",
    "                    url_proc_stack.append(url_part_string)\n",
    "                    url_proc_map[hash_code.hexdigest()] = 0\n",
    "                    \n",
    "    # Save to files before returning\n",
    "    with open('hash2url.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(hash_to_url_map, outfile, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    with open('hash2text.json', 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(hash_to_text_map, outfile, ensure_ascii=False, indent=2)    \n",
    "                    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeding steps\n",
    "seed_url = 'https://www.eenadu.net'\n",
    "root_url = ''\n",
    "md5_encode = hashlib.md5(seed_url.encode())\n",
    "\n",
    "url_process_map = {}\n",
    "# Value of 0 indicates not processed, 1 indicates processed\n",
    "url_process_map[md5_encode.hexdigest()] = 0\n",
    "\n",
    "# Keep adding and popping from this list as you get more urls\n",
    "url_process_stack = []\n",
    "url_process_stack.append(seed_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a black list of urls which return None when called on Beautiful Soup\n",
    "# Just add the blacklisted urls to the url_process_map and do not add them to the stack. It is sufficient enough \n",
    "# condition for the blacklist\n",
    "\n",
    "urls = []\n",
    "\n",
    "for u in urls:\n",
    "    md5_encode = hashlib.md5(u.encode())\n",
    "    url_process_map[md5_encode.hexdigest()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Reached crawl limit.\n"
     ]
    }
   ],
   "source": [
    "crawl_scrape(url_process_map, url_process_stack, root_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'5b43d3ef350a0d35c31ad383f61edcea': 'కావల్సినవి: చామ దుంపలు - పావు కేజీ, బియ్యప్పిండి - పెద్ద చెంచా, కారం - చెంచా, పసుపు - పావుచెంచా, ఆమ్\\u200cచూర్\\u200c పొడి - అర చెంచా, ఉప్పు - తగినంత, నూనె - వేయించేందుకు సరిపడా.\\xa0తాలింపు కోసం:మినప్పప్పు - చెంచా, ఆవాలు - అర చెంచా, ఎండుమిర్చి - రెండు, కరివేపాకు - రెబ్బ, వెల్లుల్లి రెబ్బలు - ఐదు, నూనె - చెంచా, కూరకారం - చెంచా.తయారీ: శుభ్రంగా కడిగిన చామదుంపల్ని కుక్కర్\\u200cలో వేసి ఒక కూత వచ్చేవరకూ ఉడికించుకుని తీసుకోవాలి. తరవాత చెక్కు తీసి చక్రాల్లా కోయాలి. వీటిపై బియ్యప్పిండి, కారం, పసుపు, ఆమ్\\u200cచూర్\\u200c పొడి, ఉప్పు, నూనె వేసి ఇవన్నీ ముక్కలకు పట్టేలా బాగా కలపాలి. కావాలంటే బియ్యంపిండిని మరికొంచెం కూడా కలుపుకోవచ్చు. బాణలిని పొయ్యిమీద పెట్టి నూనె వేసి ఈ ముక్కల్ని కరకరలాడేలా వేయించి తీసుకోవాలి. ఇప్పుడు తాలింపు వేసుకోవాలి. బాణలిలో నూనె వేడిచేసి మెత్తగా దంచిన వెల్లుల్లి రెబ్బలు, ఆవాలు వేయాలి. అవి చిటపటలాడాక మినప్పప్పు, ఎండుమిర్చి, కరివేపాకు వేయాలి. దీంట్లో ఇందాక వేయించి పెట్టుకున్న చామ దుంప ముక్కలు వేసి, పైన కూరకారం చల్లాలి. రెండు నిమిషాలయ్యాక దింపేయాలి.', '2b5a9cc07f518a05e5cd84783242d566': 'కావలసినవి:చిన్న ఉల్లిపాయలు: 3 కప్పులు, ఉల్లికాడల ముక్కలు: కప్పు, పచ్చిమిర్చి: ఎనిమిది, ఇంగువ: పావు టీస్పూను, పసుపు: అరటీస్పూను, ఆవాలు: టీస్పూను, మినప్పప్పు:టీస్పూను, జీలకర్ర: అరటీస్పూను, ఉప్పు: సరిపడా, నూనె: తగినంత, వేయించిన సెనగపప్పు పొడి: 4 టీస్పూన్లుతయారుచేసే విధానం:బాణలిలో నూనె పోసి కాగాక ఆవాలు, మినప్పప్పు, జీలకర్ర, పచ్చిమిర్చి వేసి వేయించాలి. తరవాత పొట్టు తీసిన ఉల్లిపాయలు వేసి వేయించాలి. వేగాక ఉప్పు, ఇంగువ, పసుపు వేసి కలపాలి. తరవాత కొద్దిగా నీళ్లు చిలకరించి ఓ ఐదు నిమిషాలు మూతపెట్టి ఉడికించాలి. ఆ తరవాత నీళ్లన్నీ ఆవిరైపోయేవరకూ వేయించాలి. ఉల్లిపాయలు బాగా వేగాక సెనగపప్పు పొడి చల్లి దించాలి.', '340557f8cb128575647e55443021e124': '\\xa0సబ్\\u200cకా మాలిక్\\u200c ఏక్\\u200c అన్న సందేశంతో యావత్\\u200c మానవాళికి శాంతి\\xa0సందేశాన్నిచ్చిన\\xa0సాయి భగవాన్\\u200c మందిరం మహారాష్ట్రలోని అహ్మద్\\u200cనగర్\\u200c జిల్లా షిర్డిలో వుంది. ఫకీర్\\u200c అవతారంలోఅనేక మహిమలు ప్రదర్శించిన సాయినాధుడు ఇప్పటికీ సమాధి నుంచే భక్తులకు అభయమిస్తాడని అసంఖ్యాక సాయి భక్తుల నమ్మకం. సాయి మందిరాన్ని దర్శించుకునేందుకు రోజు వేలాది భక్తులు షిర్డికి వస్తుంటారు.శ్రద్ధ, సబూరి\\xa0\\nశ్రద్ధ అంటే విశ్వాసం, భక్తి, సబూరి అంటే ఓర్పు, సాధన సందేశాలతో మానవాళికి అమూల్యమైన శాంతి సందేశాన్ని ఇచ్చారు. సాయినాధుడు ఎక్కడ జన్మించారు అన్న అంశంపై వేర్వేరు వాదనలు వున్నాయి. అహ్మద్\\u200cనగర్\\u200c జిల్లాలోనే 19 శతాబ్దంలో జన్మించినట్టు కొందరు పర్బానీ జిల్లాలో జన్మించినట్టు మరికొందరు పేర్కొంటారు. అయితే ఈ వాదాలను పక్కనబెడితే హిందూ, ముస్లింల మధ్య సఖ్యతకు కృషి చేసిన మహనీయుల్లో ఆయన అగ్రగణ్యుడు. షిర్డిలోని పాత మసీదు మందిరాన్నే తన నివాసంగా చేసుకొని మత సామరస్యత కోసం శ్రమించారు. ఇప్పుడు ఆ మందిరాన్ని ద్వారకామాయిగా పిలుస్తున్నారు. సమాధి మందిరం పక్కన వున్న గురుస్థానంలో ఆయన కూర్చొనివుండేవారు. తొలిసారిగా 1854లో బాలసాయిని వీక్షించిన గ్రామస్థులు ఆశ్చర్యపోయారు. ఎప్పుడూ ధ్యానంలో వుండే సాయిని అనేక ప్రశ్నలు అడిగేవారు. అనంతరం ఆయన కొంతకాలం కనిపించలేదు.ఆవో సాయి..\\xa0\\nషిర్డి గ్రామంలోని ఖండోబా మందిరంలో మహాల్సాపతి పూజరిగా వుండేవారు. ఒకసారి సాయి ఆ గ్రామంలోకి తిరిగి ప్రవేశించారు. ఆయనను చూసిన మహల్సాపతి ఆవో సాయి అని ఆహ్వానించారు. దీంతో ఆయన నామం సాయిగా స్థిరపడింది. భగవుంతునికి ఎలాంటి పేర్లు వుండవు. భక్తులు ఏ పేరుతో పిలిస్తే పలుకుతారు అదే రీతిలో సాయిబాబాగా ప్రఖ్యాతిచెందారు. సాయి మహిమలను వీక్షించిన అనేక మంది ఆయన శిష్యులుగా మారారు. మహాల్సాపతి, శ్యామ, హరి సీతారాం, దామోదర్\\u200c... తదితరులు ఆయన శిష్యగణంలో వుండేవారు. స్వామివారి మహిమలు దేశమంతటా వ్యాపించడంతో అనేకమంది భక్తులు షిర్డికి రావడం ప్రారంభించారు. 1918లో ఆయన సమాధి చెందారు. అయితే సమాధినుంచే భక్తులను అభయమిస్తుంటాను అన్న ఆయన దివ్యవ్యాఖ్యల ఫలితంగా షిర్డిక్షేత్రం భక్తజనక్షేత్రంగా మారిపోయింది.సమాధిమందిర నిర్మాణం:\\xa0బాబా భక్తులలో నాగ్\\u200cపూర్\\u200cకు చెందిన గోపాల్\\u200cరావు బూటి ఒకరు. ఆయన కలలో స్వామి కనిపించి తనకు సమాధి మందిరాన్ని నిర్మించమని కోరారు. దీంతో బూటి ఆయనకు మందిరాన్ని నిర్మించారు. అదే మనం నేడు చూస్తున్న సమాధి మందిరం. షిర్డి ప్రవేశమే అన్ని పాపాలకు పరిహారం అన్న బాబా సూక్తికి అనుగుణంగా ప్రతిరోజు వేలాదిమంది భక్తులు సాయి సన్నిధానానికి వస్తుంటారు. మందిరప్రవేశంతోనే స్వామి దివ్యమంగళ స్వరూపాన్ని వీక్షిస్తూ దివ్యానుభూతి చెందుతారు. ద్వారకామాయితో పాటు చావడి, గురుస్థానం, నందదీప్\\u200c, లెండి గార్డెన్స్\\u200c... తదితర ప్రాంతాలను మనం చూడవచ్చు. ఈ ప్రదేశాల్లో సాయి నడియాడిన అంశం మనకు గుర్తుకు వస్తే మనస్సులో ఆధ్మాత్మిక భావం అలముకుంటుంది. సాయి సంస్థాన్\\u200c వారు బాబా వస్తువులతో ప్రత్యేకంగా ఒక ప్రదర్శనశాలను ఏర్పాటుచేశారు. వీటిని కూడా వీక్షించవచ్చు.వసతి సౌకర్యం\\xa0\\n* సంస్థాన్\\u200c వారు అనేక వసతి సముదాయాలను నిర్వహిస్తున్నారు. వీటిని ఆన్\\u200cలైన్\\u200c ద్వారా ముందుగానే రిజర్వ్\\u200c చేసుకోవచ్చు,\\xa0\\n* ప్రైవేటు వసతి గృహాలు ఎక్కువగా వున్నాయి. భక్తులు వారి ఆర్థిక స్థోమతకు తగినట్టుగా గదులను తీసుకోవచ్చు.ఎలా చేరుకోవచ్చు\\xa0\\n* దేశంలోని వివిధ ప్రాంతాల నుంచి షిర్డికి రైలు, బస్సు సౌకర్యముంది.\\xa0\\n* హైదరాబాద్\\u200c నుంచి అజంతా ఎక్స్\\u200cప్రెస్\\u200c, సాయినగర్\\u200c ఎక్స్\\u200cప్రెస్\\u200cలు వున్నాయి.\\xa0\\n* అజంతా ఎక్స్\\u200cప్రెస్\\u200cలో వెళ్లేవారు దిగివలసిన స్టేషన్\\u200c నాగర్\\u200cసోల్\\u200c. అక్కడ నుంచి షిర్డికి అనేక వాహనాలు వుంటాయి.\\xa0\\n* సాయినగర్\\u200c ఎక్స్\\u200cప్రెస్\\u200cలో వెళితే నేరుగా షిర్డి చేరుకోవచ్చు.\\xa0\\n* షిర్డి సమీపంలో విమానాశ్రయాన్ని నిర్మిస్తున్నారు. త్వరలోనే సర్వీసులు ప్రారంభం కానున్నాయి.\\xa0\\n* ఔరంగాబాద్\\u200c విమానాశ్రయంలో దిగి వాహనాల ద్వారా షిర్డి చేరుకోవచ్చు.', '2ca9beb9ff0d1a729a3a90bf1634668a': 'గుంటూరు నేరవార్తలు : శీతలపానియంలో మత్తు కలిపి అత్యాచారం చేశాడని ఆరోపిస్తూ ఓ యువతి తన కుటుంబ సభ్యులతో కలసి సోమవారం అర్బన్\\u200c గ్రీవెన్స్\\u200cడేలో ఏఎస్పీ వైటీ నాయుడుకు ఫిర్యాదు చేశారు. స్పందించిన ఏఎస్పీ వెంటనే సదరు యువకుడిని పిలిపించి ఆ యువతిని వివాహం చేసుకోవాలని చెప్పాలని.. లేకపోతే అతనిపై అత్యాచారం కేసు నమోదు చేయాలని ఆదేశించారు. బాధితురాలు తెలిపిన వివరాల ప్రకారం.. కర్నూలు జిల్లాకు చెందిన ఓ యువతి గుంటూరులోని ఓ కళాశాలలో డిగ్రీ చదువుతోంది. అక్కడ వసతి గృహంలో ఉంటున్న ఆమెకు తన కుటుంబ సభ్యులు ఫోన్\\u200c చేస్తూ యోగక్షేమాలు తెలుసుకునేవాళ్లు. ఓ రోజు ఆ యువతి సోదరుడు ఆయన స్నేహితుడైన వినుకొండకు చెందిన బాలు ఫోన్\\u200c నుంచి ఆమెతో మాట్లాడాడు. అప్పటి నుంచి బాలు తరచూ ఆమెకు ఫోన్\\u200c చేసి అన్నం తిన్నావా, కళాశాలకు వెళుతున్నావా అంటుంటే ఆమె తిరస్కరించేది. దీంతో ప్రేమిస్తున్నాను..పెళ్లి చేసుకుంటాను అని నమ్మించాడు. ఓ రోజు శీతలపానియంలో మత్తు కలిపి ఇచ్చి అత్యాచారం చేశాడని ఆమె ఆరోపించింది. తాను పిలిచినప్పుడు రాకపోతే ఆ వీడియో సామాజిక మాధ్యమాల్లో, స్నేహితులకు పంపిస్తానంటూ బెదిరించేవాడని వాపోయింది. తాను గట్టిగా నిలదీస్తే పెళ్లి చేసుకుంటానని మోసం చేయనంటూ నమ్మించాడని పేర్కొంది. అతని మాటలపై నమ్మకం లేక పెద్దవాళ్లతో మాట్లాడాలన్నారు. దీంతో బాలు వాళ్లను తన నాన్న, పెద్దనాన్నల వద్దకు తీసుకువెళ్లాడు. ఆ యువతిని పెళ్లి చేసుకుంటాడని అంగీకార ఒప్పంద పత్రాలు రాసుకున్నారు. కొద్ది రోజుల తర్వాత బాలు అతని పెదనాన్నలు మాట మార్చి కట్నంగా రూ.20 లక్షలు, పొలం ఇస్తేనే పెళ్లి లేకపోతే తాళికట్టే ప్రసక్తే లేదంటున్నారని బాధితులు పేర్కొన్నారు. అదేమని అడిగినందుకు తమను దూషించి మిమ్మల్ని ఊరికి కూడా పంపించం.. ఇక్కడే చంపేస్తామని బెదిరించారని వాపోయారు. అతనితో వివాహం జరిపించి తమకు న్యాయం చేయాలని ఆమె కోరారు.\\xa0', 'cef2a3e7904727cca890ebcd2f8546f9': 'గుంటూరు నేరవార్తలు : శీతలపానియంలో మత్తు కలిపి అత్యాచారం చేశాడని ఆరోపిస్తూ ఓ యువతి తన కుటుంబ సభ్యులతో కలసి సోమవారం అర్బన్\\u200c గ్రీవెన్స్\\u200cడేలో ఏఎస్పీ వైటీ నాయుడుకు ఫిర్యాదు చేశారు. స్పందించిన ఏఎస్పీ వెంటనే సదరు యువకుడిని పిలిపించి ఆ యువతిని వివాహం చేసుకోవాలని చెప్పాలని.. లేకపోతే అతనిపై అత్యాచారం కేసు నమోదు చేయాలని ఆదేశించారు. బాధితురాలు తెలిపిన వివరాల ప్రకారం.. కర్నూలు జిల్లాకు చెందిన ఓ యువతి గుంటూరులోని ఓ కళాశాలలో డిగ్రీ చదువుతోంది. అక్కడ వసతి గృహంలో ఉంటున్న ఆమెకు తన కుటుంబ సభ్యులు ఫోన్\\u200c చేస్తూ యోగక్షేమాలు తెలుసుకునేవాళ్లు. ఓ రోజు ఆ యువతి సోదరుడు ఆయన స్నేహితుడైన వినుకొండకు చెందిన బాలు ఫోన్\\u200c నుంచి ఆమెతో మాట్లాడాడు. అప్పటి నుంచి బాలు తరచూ ఆమెకు ఫోన్\\u200c చేసి అన్నం తిన్నావా, కళాశాలకు వెళుతున్నావా అంటుంటే ఆమె తిరస్కరించేది. దీంతో ప్రేమిస్తున్నాను..పెళ్లి చేసుకుంటాను అని నమ్మించాడు. ఓ రోజు శీతలపానియంలో మత్తు కలిపి ఇచ్చి అత్యాచారం చేశాడని ఆమె ఆరోపించింది. తాను పిలిచినప్పుడు రాకపోతే ఆ వీడియో సామాజిక మాధ్యమాల్లో, స్నేహితులకు పంపిస్తానంటూ బెదిరించేవాడని వాపోయింది. తాను గట్టిగా నిలదీస్తే పెళ్లి చేసుకుంటానని మోసం చేయనంటూ నమ్మించాడని పేర్కొంది. అతని మాటలపై నమ్మకం లేక పెద్దవాళ్లతో మాట్లాడాలన్నారు. దీంతో బాలు వాళ్లను తన నాన్న, పెద్దనాన్నల వద్దకు తీసుకువెళ్లాడు. ఆ యువతిని పెళ్లి చేసుకుంటాడని అంగీకార ఒప్పంద పత్రాలు రాసుకున్నారు. కొద్ది రోజుల తర్వాత బాలు అతని పెదనాన్నలు మాట మార్చి కట్నంగా రూ.20 లక్షలు, పొలం ఇస్తేనే పెళ్లి లేకపోతే తాళికట్టే ప్రసక్తే లేదంటున్నారని బాధితులు పేర్కొన్నారు. అదేమని అడిగినందుకు తమను దూషించి మిమ్మల్ని ఊరికి కూడా పంపించం.. ఇక్కడే చంపేస్తామని బెదిరించారని వాపోయారు. అతనితో వివాహం జరిపించి తమకు న్యాయం చేయాలని ఆమె కోరారు.\\xa0', '71ee5f669cb4f617a82b358dbe861d36': 'జెర్సీపై రాజకీయ విమర్శలుముంబయి:\\xa0ప్రపంచకప్\\u200cలో భాగంగా జూన్\\u200c 30న\\xa0ఇంగ్లండ్\\u200c క్రికెట్\\u200c\\xa0జట్టుతో తలపడనున్న\\xa0టీమిండియా\\xa0కొత్త జెర్సీతో ఆడనుందని\\xa0వార్తలు వస్తున్న విషయం తెలిసిందే.\\xa0మైదానంలో\\xa0బ్లూ\\xa0కలర్\\u200c జెర్సీకి బదులు\\xa0ఆరెంజ్\\u200c\\xa0రంగు జెర్సీలతో వారు\\xa0బరిలోకి దిగే అవకాశం ఉంది.\\xa0అయితే, ఈ విషయంపై కాంగ్రెస్\\u200c, సమాజ్\\u200c వాదీ పార్టీ (ఎస్పీ)\\xa0నేతలు మండిపడుతున్నారు. దీని వెనుక కేంద్ర ప్రభుత్వ హస్తం\\xa0ఉందని ఆరోపిస్తున్నారు.\\xa0మహారాష్ట్ర ఎమ్మెల్యే, ఎస్పీ నేత\\xa0అబు అసీమ్\\u200c అజ్మి\\xa0మీడియాతో మాట్లాడుతూ... దేశం మొత్తం కాషాయీకరణ\\xa0చేసేందుకు భాజపా ప్రయత్నిస్తోందని వ్యాఖ్యానించారు.\\xa0జెండాలో మూడు రంగులు ఉంటే ఆరెంజ్\\u200cను మాత్రమే ఎందుకు తీసుకున్నారని ఆయన ప్రశ్నించారు. మహారాష్ట్ర మాజీ మంత్రి,\\xa0కాంగ్రెస్\\u200c ఎమ్మెల్యే నసీమ్\\u200c ఖాన్\\u200c ఈ విషయంపై స్పందిస్తూ.. ఎన్డీఏ ప్రభుత్వం అధికారంలోకి వచ్చినప్పటి నుంచి ఇటువంటి రాజకీయాలే చేస్తోందని, ప్రతిదాన్ని కాషాయీకరణ చేయాలని ప్రయత్నిస్తోందని వ్యాఖ్యానించారు. ప్రతిపక్షాల ఆరోపణలు, విమర్శలను\\xa0భాజపా\\xa0మిత్రపక్షాలు\\xa0కొట్టిపారేస్తున్నాయి. కేంద్రమంత్రి రామ్\\u200cదాస్\\u200c అథవాలె\\xa0ఈ విషయంపై మాట్లాడుతూ... ‘‘ఈ రంగు ధైర్యానికి, విజయానికి ప్రతీక.\\xa0దీనిపై ఎవరికీ ఎటువంటి సమస్య ఉండకూడదు’’ అని వ్యాఖ్యానించారు. శివసేన నేత, మహారాష్ట్ర మంత్రి గులాబ్\\u200c రావ్\\u200c పాటీల్\\xa0ఈ విషయంపై స్పందిస్తూ...\\xa0దీనిపై\\xa0కొందరు\\xa0అనవసర రాజకీయాలు చేస్తున్నారని విమర్శించారు. ప్రభుత్వాన్ని విమర్శించడానికి\\xa0ప్రతిపక్ష పార్టీలకు ఏ విషయమూ కనపడట్లేదని అందుకే ఆరెంజ్\\u200c\\xa0కలర్\\u200c అంటూ\\xa0ఇటువంటి విమర్శలు చేస్తున్నారని ఆయన అన్నారు. \\xa0', 'e9f169f8093fff140bf4154d8f0b6812': 'హైదరాబాద్\\u200c: ప్రముఖ సినీ దర్శకుడు వి.వి.వినాయక్\\u200cకు జీహెచ్\\u200cఎంసీ అధికారులు షాక్\\u200c ఇచ్చారు. నగర శివారులోని నార్సింగి ప్రాంతంలో అక్రమ నిర్మాణాలపై మున్సిపల్\\u200c అధికారులు ఉక్కుపాదం మోపారు. ఇందులో భాగంగా వట్టినాగులపల్లి గౌలిదొడ్డిలోని సర్వే నంబర్\\u200c 223లో రెండు అంతస్తుల భవనం కోసం అనుమతి తీసుకొని ఆరంతస్తుల్లో నిర్మించిన రెండు భవనాలను కూల్చివేశారు. అందులో వినాయక్\\u200cకు చెందిన భవనం కూడా ఉందని నార్సింగి మున్సిపల్\\u200c కమిషనర్\\u200c టి.కృష్ణమోహన్\\u200c, మేనేజర్\\u200c నర్సింహులు వెల్లడించారు.\\xa0నిర్మాణంలో ఉన్న ఓ భవనాన్ని పూర్తిగా కూల్చివేయగా.. మరో భవనంలో మూడు అంతస్తులు కూల్చివేత పూర్తయిందని తెలిపారు. మిగతా భాగాన్ని కూడా రెండు రోజుల్లో కూల్చివేస్తామని అధికారులు వివరించారు. రెండుసార్లు నోటీసులు జారీ చేసినా ఎవరూ స్పందించలేదని.. ఈ భవనాల నిర్మాణానికి అనుమతి పత్రాలు అధికారుల నుంచి కాకుండా అప్పట్లో సర్పంచ్\\u200c వద్ద నుంచి తీసుకున్నట్టు తెలిసిందన్నారు. ఈ ప్రాంతంలో మరిన్ని అక్రమ నిర్మాణాలున్నట్టు తేల్చిన మున్సిపల్\\u200c అధికారులు వాటిని కూడా కూల్చివేయనున్నారు.'}\n"
     ]
    }
   ],
   "source": [
    "# Printing the text - Just for illustration purposes\n",
    "print(hash_to_text_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
